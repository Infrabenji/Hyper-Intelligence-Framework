{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXK0kk9r8Wf4b+11Kx/y83",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Infrabenji/Workflow-working/blob/main/3_3RPcode1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv67b7oVhpFu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the environment class\n",
        "class MazeEnvironment:\n",
        "    def __init__(self, size=(5, 5), start=(0, 0), goal=(4, 4), obstacles=[]):\n",
        "        self.size = size\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.obstacles = obstacles\n",
        "        self.grid = np.zeros(size)\n",
        "        self.grid[start] = 1  # Start position\n",
        "        self.grid[goal] = 2  # Goal position\n",
        "        for obstacle in obstacles:\n",
        "            self.grid[obstacle] = -1  # Obstacles\n",
        "\n",
        "    def display(self):\n",
        "        plt.imshow(self.grid, cmap='binary')\n",
        "        plt.title('Maze Environment')\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "# Memory Encoding and Retrieval Class\n",
        "class MemorySystem:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory_storage = []\n",
        "\n",
        "    def memory_encoding(self, sensory_information):\n",
        "        sensory_information = self._flatten_information(sensory_information)  # Convert to consistent format\n",
        "        if not self.memory_storage:\n",
        "            encoded_memory = sensory_information\n",
        "        else:\n",
        "            encoded_memory = self._apply_hebbian_learning_rule(sensory_information)\n",
        "        self.memory_storage.append(encoded_memory)\n",
        "        if len(self.memory_storage) > self.capacity:\n",
        "            self.memory_storage.pop(0)\n",
        "\n",
        "    def memory_retrieval(self, retrieval_cues):\n",
        "        retrieval_cues = self._flatten_information(retrieval_cues)  # Convert to consistent format\n",
        "        return self._apply_hopfield_network(retrieval_cues)\n",
        "\n",
        "    def _flatten_information(self, information):\n",
        "        # Flattening the information into a single numpy array\n",
        "        return np.concatenate([np.ravel(np.array(info, dtype=np.float32)) for info in information])\n",
        "\n",
        "    def _apply_hebbian_learning_rule(self, input_data):\n",
        "        eta = 0.1\n",
        "        encoded_memory = []\n",
        "        input_data = np.array(input_data, dtype=np.float32)\n",
        "        for other_data in self.memory_storage:\n",
        "            other_data = np.array(other_data, dtype=np.float32)\n",
        "            min_length = min(len(input_data), len(other_data))\n",
        "            encoded_data = eta * input_data[:min_length] * other_data[:min_length]  # Ensure same length\n",
        "            encoded_memory.append(encoded_data)\n",
        "        return np.mean(encoded_memory, axis=0)  # Average encoding to maintain consistent shape\n",
        "\n",
        "    def _apply_hopfield_network(self, retrieval_cues):\n",
        "        retrieved_memory = []\n",
        "        for cue in retrieval_cues:\n",
        "            memory = self._retrieve_memory(cue)\n",
        "            retrieved_memory.append(memory)\n",
        "        return retrieved_memory\n",
        "\n",
        "    def _retrieve_memory(self, cue):\n",
        "        memory = [self._hopfield_network(cue, encoded_data) for encoded_data in self.memory_storage]\n",
        "        return memory\n",
        "\n",
        "    def _hopfield_network(self, cue, encoded_data):\n",
        "        memory = self._sign_function(np.sum(cue * encoded_data))\n",
        "        return memory\n",
        "\n",
        "    def _sign_function(self, value):\n",
        "        return 1 if value >= 0 else -1\n",
        "\n",
        "    def memory_storage_info(self):\n",
        "        print(\"Memory Storage Capacity:\", self.capacity)\n",
        "        print(\"Number of Encoded Memories:\", len(self.memory_storage))\n",
        "\n",
        "# Attentional Selection Class\n",
        "class AttentionalSelection:\n",
        "    def __init__(self):\n",
        "        self.bottom_up_attention_weights = []\n",
        "        self.top_down_attention_weights = []\n",
        "\n",
        "    def bottom_up_attention(self, stimuli, weights):\n",
        "        attention_scores = [stimulus * weight for stimulus, weight in zip(stimuli, weights)]\n",
        "        return attention_scores\n",
        "\n",
        "    def top_down_attention(self, attention_scores, priorities):\n",
        "        modulated_scores = [score * priority for score, priority in zip(attention_scores, priorities)]\n",
        "        return modulated_scores\n",
        "\n",
        "# Distress Dynamics Class\n",
        "class DistressDynamics:\n",
        "    def __init__(self, alpha, beta, gamma, xi, epsilon, M):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.xi = xi\n",
        "        self.epsilon = epsilon\n",
        "        self.M = M\n",
        "\n",
        "    def calculate_distress(self, D, C, S, W):\n",
        "        distress_change = self.alpha * D + self.beta * C + self.gamma * S + self.xi * W\n",
        "        markovian_term = self.epsilon * np.dot(self.M, D)\n",
        "        return distress_change + markovian_term\n",
        "\n",
        "    def update_distress(self, D, C, S, W, dt):\n",
        "        for _ in range(10):\n",
        "            distress_change = self.calculate_distress(D, C, S, W)\n",
        "            D += distress_change * dt\n",
        "            print(\"Distress level:\", D)\n",
        "        return D\n",
        "\n",
        "# Define the combined agent class\n",
        "class CombinedAgent:\n",
        "    def __init__(self, environment, learning_rate=0.1, discount_factor=0.9):\n",
        "        self.environment = environment\n",
        "        self.position = environment.start\n",
        "        self.q_values = np.zeros(environment.size + (4,))\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.memory_system = MemorySystem(capacity=100)\n",
        "        self.attentional_selection = AttentionalSelection()\n",
        "        self.distress_dynamics = DistressDynamics(alpha=0.2, beta=0.3, gamma=0.1, xi=0.4, epsilon=0.5, M=np.array([[0.2, 0.3, 0.5], [0.4, 0.1, 0.5], [0.1, 0.2, 0.7]]))\n",
        "\n",
        "    def choose_action(self):\n",
        "        row, col = self.position\n",
        "        possible_actions = ['up', 'down', 'left', 'right']\n",
        "        if np.random.uniform(0, 1) < 0.1:  # Exploration\n",
        "            return possible_actions[np.random.randint(len(possible_actions))]\n",
        "        else:\n",
        "            action_values = [self.q_values[row, col, self.get_action_index(action)] for action in possible_actions]\n",
        "            return possible_actions[np.argmax(action_values)]\n",
        "\n",
        "    def get_action_index(self, action):\n",
        "        action_map = {'up': 0, 'down': 1, 'left': 2, 'right': 3}\n",
        "        return action_map[action]\n",
        "\n",
        "    def step(self):\n",
        "        action = self.choose_action()\n",
        "        next_position, reward = self.take_action(action)\n",
        "        self.update_q_values(action, reward, next_position)\n",
        "        self.memory_system.memory_encoding([self.position, self.get_action_index(action), reward, next_position])\n",
        "        self.position = next_position\n",
        "        return next_position, reward\n",
        "\n",
        "    def take_action(self, action):\n",
        "        row, col = self.position\n",
        "        if action == 'up':\n",
        "            new_row, new_col = row - 1, col\n",
        "        elif action == 'down':\n",
        "            new_row, new_col = row + 1, col\n",
        "        elif action == 'left':\n",
        "            new_row, new_col = row, col - 1\n",
        "        elif action == 'right':\n",
        "            new_row, new_col = row, col + 1\n",
        "\n",
        "        # Check boundaries and obstacle positions\n",
        "        if new_row < 0 or new_row >= self.environment.size[0] or new_col < 0 or new_col >= self.environment.size[1] or (new_row, new_col) in self.environment.obstacles:\n",
        "            return (row, col), -1  # Invalid move, stay in place with penalty\n",
        "\n",
        "        if (new_row, new_col) == self.environment.goal:\n",
        "            return (new_row, new_col), 1  # Goal reached, reward = 1\n",
        "\n",
        "        return (new_row, new_col), 0  # Valid move, no reward\n",
        "\n",
        "    def update_q_values(self, action, reward, next_state):\n",
        "        row, col = self.position\n",
        "        next_row, next_col = next_state\n",
        "        action_index = self.get_action_index(action)\n",
        "        self.q_values[row, col, action_index] += self.learning_rate * (\n",
        "            reward + self.discount_factor * np.max(self.q_values[next_row, next_col]) - self.q_values[row, col, action_index]\n",
        "        )\n",
        "\n",
        "    def navigate_maze(self, num_episodes=100):\n",
        "        episode_rewards = []\n",
        "        for episode in range(num_episodes):\n",
        "            self.position = self.environment.start\n",
        "            total_reward = 0\n",
        "            while self.position != self.environment.goal:\n",
        "                next_state, reward = self.step()\n",
        "                total_reward += reward\n",
        "            episode_rewards.append(total_reward)\n",
        "        return episode_rewards\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the maze environment\n",
        "    maze_size = (5, 5)\n",
        "    start_position = (0, 0)\n",
        "    goal_position = (4, 4)\n",
        "    obstacle_positions = [(1, 2), (2, 2), (3, 2)]\n",
        "    maze = MazeEnvironment(size=maze_size, start=start_position, goal=goal_position, obstacles=obstacle_positions)\n",
        "\n",
        "    # Display the maze environment\n",
        "    maze.display()\n",
        "\n",
        "    # Create and test the combined agent\n",
        "    agent = CombinedAgent(environment=maze)\n",
        "    episode_rewards = agent.navigate_maze(num_episodes=100)\n",
        "\n",
        "    # Plot episode rewards\n",
        "    plt.plot(episode_rewards)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.title('Agent Performance')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MazeEnvironment:\n",
        "    def __init__(self):\n",
        "        self.grid = np.zeros((5, 5))\n",
        "        self.grid[0, 0] = 0.75  # Gray square in top left\n",
        "        self.grid[4, 4] = 2.0   # Black square in bottom right\n",
        "        self.grid[1:4, 2] = -1.0  # White squares in the middle\n",
        "\n",
        "    def render(self):\n",
        "        plt.imshow(self.grid, cmap='gray', vmin=-1, vmax=2)\n",
        "        plt.colorbar()\n",
        "        plt.title(\"Maze Environment\")\n",
        "        plt.show()\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.position = (0, 0)\n",
        "        self.total_reward = 0\n",
        "        self.history = []\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = self.position\n",
        "        if action == 'up' and row > 0:\n",
        "            row -= 1\n",
        "        elif action == 'down' and row < 4:\n",
        "            row += 1\n",
        "        elif action == 'left' and col > 0:\n",
        "            col -= 1\n",
        "        elif action == 'right' and col < 4:\n",
        "            col += 1\n",
        "\n",
        "        self.position = (row, col)\n",
        "        reward = self.env.grid[row, col]\n",
        "        self.total_reward += reward\n",
        "        self.history.append((self.position, reward))\n",
        "        return reward\n",
        "\n",
        "    def render(self):\n",
        "        env_copy = self.env.grid.copy()\n",
        "        row, col = self.position\n",
        "        env_copy[row, col] = 1.5  # Indicate agent's position\n",
        "        plt.imshow(env_copy, cmap='gray', vmin=-1, vmax=2)\n",
        "        plt.colorbar()\n",
        "        plt.title(\"Maze Environment with Agent\")\n",
        "        plt.show()\n",
        "\n",
        "# Initialize environment and agent\n",
        "env = MazeEnvironment()\n",
        "agent = Agent(env)\n",
        "\n",
        "# Render the environment\n",
        "env.render()\n",
        "agent.render()\n",
        "\n",
        "# Sample agent performance\n",
        "performance = []\n",
        "for episode in range(100):\n",
        "    agent.position = (0, 0)\n",
        "    agent.total_reward = 0\n",
        "    for _ in range(10):  # Assume 10 steps per episode\n",
        "        action = np.random.choice(['up', 'down', 'left', 'right'])\n",
        "        agent.step(action)\n",
        "    performance.append(agent.total_reward)\n",
        "\n",
        "# Plot performance\n",
        "plt.plot(performance)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Agent Performance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LUY6IbelhtM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentWithRE(Agent):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def compute_decision_function(self, sense_data, actions):\n",
        "        best_action = None\n",
        "        max_fitness = float('-inf')\n",
        "        for action in actions:\n",
        "            expected_fitness = 0\n",
        "            for s in range(5):\n",
        "                for s_prime in range(5):\n",
        "                    prob_s = 1/5  # Simplified probability\n",
        "                    prob_d_given_s = 1/5\n",
        "                    prob_s_prime_given_s_and_a = 1/5\n",
        "                    v_s_s_prime = self.env.grid[s_prime, s_prime]\n",
        "                    expected_fitness += prob_s * prob_d_given_s * prob_s_prime_given_s_and_a * v_s_s_prime\n",
        "            if expected_fitness > max_fitness:\n",
        "                max_fitness = expected_fitness\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def step_with_re(self):\n",
        "        actions = ['up', 'down', 'left', 'right']\n",
        "        sense_data = self.position\n",
        "        action = self.compute_decision_function(sense_data, actions)\n",
        "        return self.step(action)\n",
        "\n",
        "# Initialize environment and agent with RE\n",
        "agent_re = AgentWithRE(env)\n",
        "\n",
        "# Sample agent performance with RE\n",
        "performance_re = []\n",
        "for episode in range(100):\n",
        "    agent_re.position = (0, 0)\n",
        "    agent_re.total_reward = 0\n",
        "    for _ in range(10):  # Assume 10 steps per episode\n",
        "        agent_re.step_with_re()\n",
        "    performance_re.append(agent_re.total_reward)\n",
        "\n",
        "# Plot performance with RE\n",
        "plt.plot(performance_re)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Agent Performance with RE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lbVd9m-1hyyd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}